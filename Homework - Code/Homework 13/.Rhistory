corpus <- tm_map(corpus, removeWords, stopwords("english"))
# Show the content of the first document
corpus[[1]]$content %>%
strwrap() %>% head()
# remove punctuation from the document collection
corpus <- tm_map(corpus, removePunctuation)
# Show the content of the first document
corpus[[1]]$content %>%
strwrap() %>% head()
# Stemming
corpus <- tm_map(corpus, stemDocument, language = "english")
# Show the content of the first document
corpus[[1]]$content %>%
strwrap() %>% head()
library(wordcloud)
library(RColorBrewer)
# Word cloud for the whole copus (40 documents)
wordcloud(corpus,
max.words = 150,
random.order = FALSE,
colors = brewer.pal(8, "Dark2"))
# Create terms-documents matrix
tdm <- TermDocumentMatrix(corpus)
inspect(tdm)
# Find top terms that occur at least 50 times
top_words <- findFreqTerms(tdm, 50)
top_words
# Create a TF-IDF matrix using the top words
tfidf <- DocumentTermMatrix(corpus,
control = list(dictionary = top_words,
weighting = weightTfIdf))
inspect(tfidf)
# Convert document-term matrix as data frame
df <- tfidf %>% as.matrix() %>% as.data.frame()
# Show the head of the first 10 columns
head(df[,1:10])
df$spamham <- file_summary$file_name
# Define ham (spamham = 1) and spam (spamham = 0)
df$spamham <- ifelse((df$spamham == "ham"), 1, 0)
df$spamham <- factor(df$spamham,
levels = c(0,1),
labels = c("SPAM","HAM"))
head(df)
# Show the summary of the last 20 columns
summary(df[,(length(df)-19):length(df)])
library(caret)
set.seed(1234)
trainIndex <- createDataPartition(df$spamham, p = .3, list = FALSE)
head(trainIndex)
train_data <- df[trainIndex,]
test_data <- df[-trainIndex,]
m1 <- lm(spamham ~ ., data = train_data)
# Make predictions
x_test <- test_data[,-163]
y_test <- test_data[,163]
predictions <- predict(m1, test_data)
# Summarize results
postResample(predictions,y_test)
## Train a logistic regression model using 10-fold cross-validation
fitControl <- trainControl(method = "cv",number = 10)
set.seed(123)
logit_fit <- train(spamham ~ ., data = df,
trControl = fitControl,
method="glm", family=binomial(link='logit'),
control = list(maxit = 50))
print(logit_fit)
confusionMatrix(logit_fit)
# Train Naive Bayes with 10-fold Cross-Validation
set.seed(123)
nn_fit <- train(spamham ~ ., data = df,
trControl = fitControl,
preProcess = c("center", "scale"),
method = "naive_bayes")
print(nn_fit)
confusionMatrix(nn_fit)
# Collect resamples
resamps <- resamples(list(Logit=logit_fit, NaiveBayes = nn_fit))
summary(resamps)
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls())
library(magrittr)
library(dplyr)
doc <- read.table(file = "SMSSpamCollection.txt",
header = FALSE,
sep = "\t",
quote = "",
fill = TRUE,
col.names = c("Type","Text"),
stringsAsFactors=FALSE
)
doc$ID <- seq.int(nrow(doc))
library(tm)
docs <- data.frame(doc_id = c(doc$Type),
text = c(doc$Text),
stringsAsFactors = FALSE)
corpus <- Corpus(DataframeSource(docs))
corpus <- corpus[-c(5575)]
file_summary <- corpus %>%
summary() %>%
as.data.frame() %>%
filter(Var2=="Length") %>%
select(Var1) %>% rename(file_name = Var1)
# Show the content of the first document
corpus[[1]]$content %>%
strwrap() %>% head()
corpus <- tm_map(corpus, stripWhitespace)
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls())
library(magrittr)
library(dplyr)
doc <- read.table(file = "SMSSpamCollection.txt",
header = FALSE,
sep = "\t",
quote = "",
fill = TRUE,
col.names = c("Type","Text"),
stringsAsFactors=FALSE
)
doc$ID <- seq.int(nrow(doc))
library(tm)
docs <- data.frame(doc_id = c(doc$Type),
text = c(doc$Text),
stringsAsFactors = FALSE)
corpus <- Corpus(DataframeSource(docs))
corpus <- corpus[-c(5575)]
file_summary <- corpus %>%
summary() %>%
as.data.frame() %>%
filter(Var2=="Length") %>%
select(Var1) %>% rename(file_name = Var1)
# Show the content of the first document
corpus[[1]]$content %>%
strwrap() %>% head()
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, removeNumbers)
corpus = tm_map(corpus,content_transformer(function(x) iconv(x, to='ASCII', sub='')))
# remove English stopwords from the document collection
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, removePunctuation)
# Show the content of the first document
corpus[[1]]$content %>%
strwrap() %>% head()
corpus <- tm_map(corpus, stemDocument, language = "english")
library(wordcloud)
library(RColorBrewer)
# Word cloud for the whole copus (40 documents)
wordcloud(corpus,
max.words = 150,
random.order = FALSE,
colors = brewer.pal(8, "Dark2"))
# Find top terms that occur at least 50 times
top_words <- findFreqTerms(tdm, 50)
# Create terms-documents matrix
tdm <- TermDocumentMatrix(corpus)
inspect(tdm)
top_words <- findFreqTerms(tdm, 50)
top_words
tfidf <- DocumentTermMatrix(corpus,
control = list(dictionary = top_words,
weighting = weightTfIdf))
inspect(tfidf)
df <- tfidf %>% as.matrix() %>% as.data.frame()
# Show the head of the first 10 columns
head(df[,1:10])
df$spamham <- file_summary$file_name
# Define ham (spamham = 1) and spam (spamham = 0)
df$spamham <- ifelse((df$spamham == "ham"), 1, 0)
df$spamham <- factor(df$spamham,
levels = c(0,1),
labels = c("SPAM","HAM"))
head(df)
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls())
library(magrittr)
library(dplyr)
doc <- read.table(file = "SMSSpamCollection.txt",
header = FALSE,
sep = "\t",
quote = "",
fill = TRUE,
col.names = c("Type","Text"),
stringsAsFactors=FALSE
)
doc$ID <- seq.int(nrow(doc))
library(tm)
docs <- data.frame(doc_id = c(doc$Type),
text = c(doc$Text),
stringsAsFactors = FALSE)
corpus <- Corpus(DataframeSource(docs))
corpus <- corpus[-c(5575)]
file_summary <- corpus %>%
summary() %>%
as.data.frame() %>%
filter(Var2=="Length") %>%
select(Var1) %>% rename(file_name = Var1)
# Show the content of the first document
corpus[[1]]$content %>%
strwrap() %>% head()
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, removeNumbers)
corpus = tm_map(corpus,content_transformer(function(x) iconv(x, to='ASCII', sub='')))
# remove English stopwords from the document collection
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, removePunctuation)
# Show the content of the first document
corpus[[1]]$content %>%
strwrap() %>% head()
corpus <- tm_map(corpus, stemDocument, language = "english")
library(wordcloud)
library(RColorBrewer)
# Word cloud for the whole copus (40 documents)
wordcloud(corpus,
max.words = 150,
random.order = FALSE,
colors = brewer.pal(8, "Dark2"))
# Create terms-documents matrix
tdm <- TermDocumentMatrix(corpus)
inspect(tdm)
top_words <- findFreqTerms(tdm, 50)
top_words
tfidf <- DocumentTermMatrix(corpus,
control = list(dictionary = top_words,
weighting = weightTfIdf))
inspect(tfidf)
df <- tfidf %>% as.matrix() %>% as.data.frame()
# Show the head of the first 10 columns
head(df[,1:10])
df$spamham <- file_summary$file_name
# Define ham (spamham = 1) and spam (spamham = 0)
df$spamham <- ifelse((df$spamham == "ham"), 1, 0)
df$spamham <- factor(df$spamham,
levels = c(0,1),
labels = c("SPAM","HAM"))
head(df)
library(caret)
set.seed(1234)
trainIndex <- createDataPartition(df$spamham, p = .3, list = FALSE)
head(trainIndex)
train_data <- df[trainIndex,]
test_data <- df[-trainIndex,]
m1 <- lm(spamham ~ ., data = train_data)
# Make predictions
x_test <- test_data[,-163]
y_test <- test_data[,163]
predictions <- predict(m1, test_data)
# Summarize results
postResample(predictions,y_test)
fitControl <- trainControl(method = "cv",number = 10)
set.seed(123)
logit_fit <- train(spamham ~ ., data = df,
trControl = fitControl,
method="glm", family=binomial(link='logit'),
control = list(maxit = 50))
print(logit_fit)
confusionMatrix(logit_fit)
# Train Naive Bayes with 10-fold Cross-Validation
set.seed(123)
nn_fit <- train(spamham ~ ., data = df,
trControl = fitControl,
preProcess = c("center", "scale"),
method = "naive_bayes")
print(nn_fit)
confusionMatrix(nn_fit)
# Collect resamples
resamps <- resamples(list(Logit=logit_fit, NaiveBayes = nn_fit))
summary(resamps)
install.packages("AUC")
library(AUC)
library(AUC)
auc(nn_fit)
auc(nn_fit)
auc(nn_fit, min = 0, max = 1)
install.packages("pROC")
library(pROC)
auc(df$spamham)
auc(df$spamham, df)
auc(df$spamham, -df$spamham)
library(data.table)
install.packages("mltools")
library(mltools)
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())
library(tm)
df <- read.delim("SMSSpamCollection.txt", sep="\t", header = FALSE, col.names = c("Class", "Text"), encoding = "utf-8", quote = "")
df$ID <- 1:nrow(df)
corpus <- Corpus(VectorSource(df$Text))
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stemDocument, language = "english")
library(wordcloud)
library(RColorBrewer)
wordcloud(corpus, max.words = 75, random.order = FALSE, colors = brewer.pal(8, "Dark2"))
library(magrittr)
# Create term document matrix
tdm <- TermDocumentMatrix(corpus)
# Create list with top 50 words
topWords <- findFreqTerms(tdm, 50)
# Create document term matrix
dtm <- DocumentTermMatrix(corpus, control = list(dictionary = topWords, weighting = weightTfIdf))
# Convert to a dataframe
matrixdf <- dtm %>% as.matrix %>% as.data.frame()
# Add column to predict back to dataframe
matrixdf$hamspam <- df$Class
library(caret)
set.seed(123)
trainIndex <- createDataPartition(matrixdf$hamspam, p = .3, list = FALSE)
trainData <- matrixdf[ trainIndex,]
testData <- matrixdf[-trainIndex,]
fitControl <- trainControl(method = "cv", number = 10, classProbs = TRUE, summaryFunction = twoClassSummary)
naiveBayesFit <- train(hamspam ~ .,
data = trainData,
trControl = fitControl,
preProcess = c("center", "scale"),
method = "naive_bayes",
metric = "ROC")
print(naiveBayesFit)
logisticFit <- train(hamspam ~ .,
data = trainData,
trControl = fitControl,
method = "glm",
family = binomial(link = 'logit'),
metric = "ROC")
print(logisticFit)
confusionNaiveBayes <- confusionMatrix(naiveBayesFit)
confusionLogistic <- confusionMatrix(logisticFit)
print(confusionNaiveBayes)
print(confusionLogistic)
library(wordcloud)
library(RColorBrewer)
wordcloud(corpus, max.words = 75, random.order = FALSE, colors = brewer.pal(8, "Set1"))
rm(list=ls())
library(tm)
df <- read.delim("SMSSpamCollection.txt", sep="\t", header = FALSE, col.names = c("Class", "Text"), encoding = "utf-8", quote = "")
df$ID <- 1:nrow(df)
corpus <- Corpus(VectorSource(df$Text))
corpus <- tm_map(corpus, stripWhitespace) %>%
tm_map(corpus, tolower) %>%
tm_map(corpus, removeNumbers) %>% tm_map(corpus, removePunctuation) %>%
tm_map(corpus, removeNumbers) %>% tm_map(corpus, removeWords, stopwords("english")) %>%
tm_map(corpus, stemDocument, language = "english")
corpus <- corpus %>% tm_map(stripWhitespace) %>%
tm_map(corpus, tolower) %>%
tm_map(removeNumbers) %>%
tm_map(removePunctuation) %>%
tm_map(removeNumbers) %>% tm_map(corpus, removeWords, stopwords("english")) %>%
tm_map(stemDocument, language = "english")
knitr::opts_chunk$set(echo = TRUE, message = FALSE)
rm(list=ls())
library(magrittr)
library(dplyr)
library(tm)
df <- read.delim("SMSSpamCollection.txt", sep="\t", header = FALSE, col.names = c("Class", "Text"), encoding = "utf-8", quote = "")
df$ID <- 1:nrow(df)
corpus <- Corpus(VectorSource(df$Text))
corpus <- corpus %>% tm_map(stripWhitespace) %>%
tm_map(corpus, tolower) %>%
tm_map(removeNumbers) %>%
tm_map(removePunctuation) %>%
tm_map(removeNumbers) %>% tm_map(corpus, removeWords, stopwords("english")) %>%
tm_map(stemDocument, language = "english")
corpus <- corpus %>% tm_map(stripWhitespace) %>%
tm_map(corpus, tolower) %>%
tm_map(removeNumbers) %>%
tm_map(removePunctuation) %>%
tm_map(removeNumbers) %>%
tm_map(removeWords, stopwords("english")) %>%
tm_map(stemDocument, language = "english")
knitr::opts_chunk$set(echo = TRUE)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stemDocument, language = "english")
# Load packages
library(wordcloud)
library(RColorBrewer)
# Create wordcloud
wordcloud(corpus, max.words = 75, random.order = FALSE, colors = brewer.pal(8, "Set1"))
library(magrittr)
# Create term document matrix
tdm <- TermDocumentMatrix(corpus)
# Create list with words that appear 50 times
topWords <- findFreqTerms(tdm, 50)
# Create document term matrix
dtm <- DocumentTermMatrix(corpus, control = list(dictionary = topWords, weighting = weightTfIdf))
# Convert to a dataframe
matrixdf <- dtm %>% as.matrix %>% as.data.frame()
# Add column to predict back to dataframe
matrixdf$hamspam <- df$Class
library(caret)
set.seed(123)
trainIndex <- createDataPartition(matrixdf$hamspam, p = .3, list = FALSE)
trainData <- matrixdf[ trainIndex,]
testData <- matrixdf[-trainIndex,]
fitControl <- trainControl(method = "cv", number = 10, classProbs = TRUE, summaryFunction = twoClassSummary)
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())
library(tm)
df <- read.delim("SMSSpamCollection.txt", sep="\t", header = FALSE, col.names = c("Class", "Text"), encoding = "utf-8", quote = "")
df$ID <- 1:nrow(df)
corpus <- Corpus(VectorSource(df$Text))
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stemDocument, language = "english")
# Load packages
library(wordcloud)
library(RColorBrewer)
# Create wordcloud
wordcloud(corpus, max.words = 75, random.order = FALSE, colors = brewer.pal(8, "Set1"))
library(magrittr)
# Create term document matrix
tdm <- TermDocumentMatrix(corpus)
# Create list with words that appear 50 times
topWords <- findFreqTerms(tdm, 50)
# Create document term matrix
dtm <- DocumentTermMatrix(corpus, control = list(dictionary = topWords, weighting = weightTfIdf))
# Convert to a dataframe
matrixdf <- dtm %>% as.matrix %>% as.data.frame()
# Add column to predict back to dataframe
matrixdf$hamspam <- df$Class
library(caret)
set.seed(123)
trainIndex <- createDataPartition(matrixdf$hamspam, p = .3, list = FALSE)
trainData <- matrixdf[ trainIndex,]
testData <- matrixdf[-trainIndex,]
fitControl <- trainControl(method = "cv", number = 10, classProbs = TRUE, summaryFunction = twoClassSummary)
logisticFit <- train(hamspam ~ .,
data = trainData,
trControl = fitControl,
method = "glm",
family = binomial(link = 'logit'),
metric = "ROC")
print(logisticFit)
confusionMatrix(logisticFit)
naiveBayesFit <- train(hamspam ~ .,
data = trainData,
trControl = fitControl,
preProcess = c("center", "scale"),
method = "naive_bayes",
metric = "ROC")
print(naiveBayesFit)
confusionMatrix(naiveBayesFit)
logisticFit <- train(hamspam ~ .,
data = trainData,
trControl = fitControl,
method = "glm",
family = binomial(link = 'logit'),
metric = "ROC")
print(logisticFit)
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())
library(tm)
df <- read.delim("SMSSpamCollection.txt", sep="\t", header = FALSE, col.names = c("Class", "Text"), encoding = "utf-8", quote = "")
df$ID <- 1:nrow(df)
corpus <- Corpus(VectorSource(df$Text))
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stemDocument, language = "english")
# Load packages
library(wordcloud)
library(RColorBrewer)
# Create wordcloud
wordcloud(corpus, max.words = 75, random.order = FALSE, colors = brewer.pal(8, "Set1"))
library(magrittr)
# Create term document matrix
tdm <- TermDocumentMatrix(corpus)
# Create list with words that appear 50 times
topWords <- findFreqTerms(tdm, 50)
# Create document term matrix
dtm <- DocumentTermMatrix(corpus, control = list(dictionary = topWords, weighting = weightTfIdf))
# Convert to a dataframe
matrixdf <- dtm %>% as.matrix %>% as.data.frame()
# Add column to predict back to dataframe
matrixdf$hamspam <- df$Class
library(caret)
set.seed(123)
trainIndex <- createDataPartition(matrixdf$hamspam, p = .3, list = FALSE)
trainData <- matrixdf[ trainIndex,]
testData <- matrixdf[-trainIndex,]
fitControl <- trainControl(method = "cv", number = 10, classProbs = TRUE, summaryFunction = twoClassSummary)
logisticFit <- train(hamspam ~ .,
data = trainData,
trControl = fitControl,
method = "glm",
family = binomial(link = 'logit'),
metric = "ROC")
print(logisticFit)
confusionMatrix(logisticFit)
naiveBayesFit <- train(hamspam ~ .,
data = trainData,
trControl = fitControl,
preProcess = c("center", "scale"),
method = "naive_bayes",
metric = "ROC")
print(naiveBayesFit)
confusionMatrix(naiveBayesFit)
